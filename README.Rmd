---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  eval = FALSE
)
```

# twittercache

<!-- badges: start -->
[![Travis build status](https://travis-ci.org/alexpghayes/twittercache.svg?branch=master)](https://travis-ci.org/alexpghayes/twittercache)
<!-- badges: end -->

`twittercache` facilitates robust sampling of the Twitter graph. The basic idea is to save any data into a local cache as you as you get it. `twittercache` is build on top of [`socialsampler`](https://github.com/alexpghayes/socialsampler) and [`rtweet`](https://rtweet.info/).

Please see the `socialsampler` documentation for information on how to register Twitter tokens.

## Installation

You can install the development version of `twittercache` with:

``` r
install.packages("devtools")
devtools::install_github("alexpghayes/twittercache")
```

## `rtweet` replacements

- Use `cache_lookup_users()` instead of `lookup_users()`
- Use `cache_get_friends()` instead of `get_friends()`
- Use `cache_get_followers()` instead of `get_followers()`


## Frequently asked questions

**How do you manage the API rate limits?**

Currently we just sample one node a minute. This complies with the Twitter API rate limits, but only if you don't use the registered tokens for anything else while sampling.

**How efficient is this whole process?**

We currently sample at about a maximum rate of 60 users / minute. So if you have ~60 or fewer tokens, we sample as fast as possible while respecting the Twitter API rate limits. 

We can push this arbritrarily high (provided you have the tokens) by parallelizing a key loop in `sample_twitter_graph()`. I don't have 60 tokens so I'm not worrying about this yet, but let me know if this is something you need.

**Where is the cache itself?**

It's at `~/.twittergraph/`.

**Why is the cache so large?**

Twitter user IDs are 64 bit integers, which are expensive to store. Also, the Twitter graph is super dense, which makes storing the edgelist painful.

**This whole cache thing is a bad idea**

Probably, but it works for me.

**What about users with big follower counts? Do you sample all the followers?**

No. We only sample up to 5,000 friends and 5,000 followers. This is mostly because I'm lazy and also it's wasteful to spend API calls on someone with tons of followers.

Recall that you can pick up an edge in the network from either node. We recommend trying to make sure the node with smaller degree is in your sample.

**What happens when I try to sample protected users?**

We remove these users from the sample at export time.

**What is this mess of weird logger calls?**

If someone knows better practices for logging things, please reach out, I really have no clue what I'm doing here.

## Contributing

Please note that the `twittercache` project is released with a [Contributor Code of Conduct](CODE_OF_CONDUCT.md).

By contributing to this project, you agree to abide by its terms.
